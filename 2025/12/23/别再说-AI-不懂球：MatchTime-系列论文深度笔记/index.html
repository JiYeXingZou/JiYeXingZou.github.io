
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>别再说 AI 不懂球：MatchTime 系列论文深度笔记 | Izumi Sagiri</title>
    <meta name="author" content="Izumi Sagiri" />
    <meta name="description" content="分享平时的技术和生活" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar.jpg" />

    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />
 
<meta name="generator" content="Hexo 7.2.0"><link rel="alternate" href="/atom.xml" title="Izumi Sagiri" type="application/atom+xml">
</head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>IZUMI SAGIRI</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;IZUMI SAGIRI</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>别再说 AI 不懂球：MatchTime 系列论文深度笔记</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/12/23
        </span>
        
        <span class="category">
            <a href="/categories/%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                学术研究
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="color: #03a9f4">
                    论文笔记
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/MatchTime/" style="color: #00bcd4">
                    MatchTime
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/SoccerNet/" style="color: #ffa2c4">
                    SoccerNet
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/%E8%B6%B3%E7%90%83AI/" style="color: #00a596">
                    足球AI
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h1>别再说 AI 不懂球：MatchTime 系列论文深度笔记（一）</h1>
<h2 id="核心痛点：消失的“16秒”">核心痛点：消失的“16秒”</h2>
<p>在足球视频领域，传统的解说词数据存在严重的<strong>音画不同步</strong>。由于原始数据大多抓取自实时文字直播，解说词往往比实际进球画面晚了 <strong>16.63 秒</strong>甚至更多。如果直接用这种“脏数据”训练，AI 只会学会“马后炮”。</p>
<hr>
<h2 id="第一章：数据治本——如何通过数学实现“降维打击”">第一章：数据治本——如何通过数学实现“降维打击”</h2>
<p>论文的核心贡献在于 <strong>Section 3.2 (Temporal Alignment Pipeline)</strong>，它展示了如何用一套自动化的管线将滞后的文本“拽”回正确的帧。</p>
<h3 id="1-亲和力矩阵-Affinity-Matrix-：连连看的底牌">1. 亲和力矩阵 (Affinity Matrix)：连连看的底牌</h3>
<p>为了对齐视频帧（Visual Frames）和解说词（Text Captions），作者引入了 <strong>Affinity Matrix  $\mathbb{A}$</strong>。</p>
<ul>
<li><strong>它是怎么来的？</strong> 假设视频有$n$ 帧，文本有 $k$ 句，矩阵的大小就是$k \times n$ 。</li>
</ul>
<h3 id="核心公式推导：亲和力矩阵-Affinity-Matrix">核心公式推导：亲和力矩阵 (Affinity Matrix)</h3>
<p>在计算视频帧 $V_j$ 与文本 $C_i$ 的相似度时，公式表达为：</p>
<p>$$\mathbb{A}[i, j] = \frac{C_i \cdot V_j^T}{||C_i|| \cdot ||V_j|| \cdot \tau}$$</p>
<p><strong>公式拆解：</strong></p>
<ul>
<li><strong>分子 ($C_i \cdot V_j^T$)</strong>：特征向量的点积，衡量方向一致性。</li>
<li><strong>分母 ($||C_i|| \cdot ||V_j||$)</strong>：$L_2$ 范数归一化，确保计算的是<strong>余弦相似度</strong>。</li>
<li><strong>$\tau$</strong>：温度参数，用来控制相似度分布的平滑度。</li>
<li><strong>深度解析</strong>：</li>
<li>** 是什么？** 这是向量的 ** 范数**（模长）。之所以要除以它，是为了进行<strong>归一化</strong>，将计算锁定为<strong>余弦相似度</strong>。我们只关心文本和画面的“语义方向”是否一致，而不关心特征向量本身的绝对大小。</li>
<li><strong>为什么要算这个？</strong> 通过寻找矩阵每一行中的最大值（），模型能自动锁死每一句解说词对应的“高光时刻”。</li>
</ul>
<hr>
<h2 id="第二章：模型架构——MatchVoice-的“翻译”逻辑">第二章：模型架构——MatchVoice 的“翻译”逻辑</h2>
<p>MatchVoice 的本质是一个<strong>多模态大模型 (MLLM)</strong>。它通过一套精密设计的组件，将视频“翻译”成文字。</p>
<h3 id="1-为什么视觉编码器-Visual-Encoder-要冻结？">1. 为什么视觉编码器 (Visual Encoder) 要冻结？</h3>
<p>在架构图中，你会看到视觉部分（如 CLIP 或 Baidu 特征）被打上了“雪花”图标（Frozen）。</p>
<ul>
<li><strong>策略</strong>：冻结预训练好的编码器可以保持其强大的通用特征提取能力，同时大幅降低训练成本。</li>
</ul>
<h3 id="2-Learnable-Queries-Attention：精准探测器">2. Learnable Queries &amp; Attention：精准探测器</h3>
<ul>
<li><strong>Learnable Queries</strong>：它们不是来自视频，而是模型内置的“探测员”。</li>
<li><strong>自注意力 (Self-Attention)</strong>：让这群“探测员”在出发前先开个会，分工合作（比如有的看人，有的看球）。</li>
<li><strong>交叉注意力 (Cross-Attention)</strong>：这是关键！探测员拿着清单去视频特征（超市货架）里取货，把散落在时空中的信息吸收到 Query 向量中。<br>
<img src="/2025/12/23/%E5%88%AB%E5%86%8D%E8%AF%B4-AI-%E4%B8%8D%E6%87%82%E7%90%83%EF%BC%9AMatchTime-%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%B7%B1%E5%BA%A6%E7%AC%94%E8%AE%B0/image-4.png" alt="alt text"></li>
</ul>
<h3 id="3-从投影到生成：MLP-与-Prefix-Tokens">3. 从投影到生成：MLP 与 Prefix Tokens</h3>
<ul>
<li><strong>MLP 的翻译官作用</strong>：视觉特征与 LLM 的维度往往不匹配。<strong>MLP (Projection Layer)</strong> 就像转换插头，将视觉特征投影到 LLM 能听懂的空间。</li>
<li><strong>Prefix Tokens (紫色方块)</strong>：这是 MLP 输出的唯一成果。它们作为“视觉前缀”喂给 LLM（如 LLaMA-3）。</li>
<li><strong>生成逻辑:$C = \Psi_{dec}(\Psi_{proj}(F))$。</strong>：。LLM 接收到视觉前缀后，开始顺着这个背景一个词一个词地吐出蓝色的 <strong>Commentary Tokens</strong>。</li>
</ul>
<hr>
<h2 id="第三章：评估的“金标准”——SN-Caption-test-align">第三章：评估的“金标准”——SN-Caption-test-align</h2>
<p>为了证明 AI 真的看懂了球，作者没有使用模糊的原始数据进行评估，而是打造了 <strong>SN-Caption-test-align</strong>。</p>
<ul>
<li><strong>本质</strong>：这是对 SoccerNet-Caption 的<strong>人工精修版</strong>。</li>
<li><strong>意义</strong>：它不仅是一个数据集，更是一个“公正的考官”。只有在时间戳绝对准确的考卷上拿到高分（如 <strong>CIDEr</strong> 分数的暴涨），才能证明对齐管线的有效性。</li>
</ul>
<hr>
<ul>
<li><strong>Baidu 特征最强</strong>：实验证明，相比通用的 CLIP，这种在足球领域“深造”过的模型（Baidu Soccer Embeddings）作为视觉编码器效果最佳。</li>
<li><strong>数据 &gt; 模型</strong>：即便使用基础的 ResNet，只要用了对齐后的 MatchTime 数据集，表现甚至能超越在脏数据上跑的高级模型。</li>
</ul>
<hr>
<h2 id="第二篇论文-Towards-Universal-Soccer-Video-Understanding">第二篇论文 Towards Universal Soccer Video Understanding</h2>
<p>第二篇论文在第一篇论文的基础上提出了SoccerReplay-1988数据集。<br>
并且提出了足球专用的解码器MatchVison</p>
<h3 id="SoccerReplay-19886-Dataset">SoccerReplay-19886 Dataset</h3>
<p>这篇文章阐述了这个作者是如何做这个数据集的，将视频分为上下两场，从starting at kick off开始。并且采用第一篇文章的MatchTime的对齐方式，通过手动进行人工对齐</p>
<p><img src="/2025/12/23/%E5%88%AB%E5%86%8D%E8%AF%B4-AI-%E4%B8%8D%E6%87%82%E7%90%83%EF%BC%9AMatchTime-%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%B7%B1%E5%BA%A6%E7%AC%94%E8%AE%B0/image-5.png" alt="alt text"></p>
<h3 id="对于模型我自己的理解">对于模型我自己的理解:</h3>
<p>本质上就是一个改进的video transformer</p>
<h4 id="Token-Embedding">Token Embedding</h4>
<p><strong>空间切分：</strong> 每一帧图像 $ i $ 被切分成 $ M $ 个不重叠的小方块（Patches）。这就像把一张照片剪成方格阵列。</p>
<p><strong>线性映射 ($\Phi_{Emb}$)：</strong> 每个小方块被拉平并转换成一个维度为 $D$ 的向量。</p>
<p><strong>双重位置编码 (Position Embedding)：</strong> 这是关键。</p>
<ul>
<li><strong>空间位置编码 ($e_s^{pos}$)</strong>：告诉模型这个方块在画面的哪个位置（左上还是右下）。</li>
<li><strong>时间位置编码 ($e_t^{pos}$)</strong>：在处理完整个视频序列后叠加，告诉模型这一组特征属于视频的第几秒。</li>
</ul>
<p><strong>[CLS] 标记：</strong> 每一帧都会加入一个特殊的 <code>[cls]</code> 标记，专门用来汇总这一帧的全局信息。</p>
<h4 id="时空注意力模块">时空注意力模块</h4>
<p><strong>时间自注意力 ($\phi_t$)：</strong></p>
<ul>
<li><strong>操作：</strong> 只在不同帧的<strong>相同空间位置</strong>的 Token 之间进行计算。</li>
<li><strong>目的：</strong> 追踪动作。例如，第1帧里的足球在左边，第2帧里的足球移动到了中间，时间注意力负责捕捉这个“移动”轨迹。</li>
</ul>
<p><strong>空间自注意力 ($\phi_s$)：</strong></p>
<ul>
<li><strong>操作：</strong> 只在<strong>同一帧内部</strong>的不同 Token 之间进行计算。</li>
<li><strong>目的：</strong> 理解布局。例如，识别出这一帧画面里哪是球员、哪是球门、哪是裁判。</li>
</ul>
<p><strong>交替堆叠 ($K$ 次)：</strong> 通过多次交替循环，模型既能看清每一帧的细节，又能理解动作在时间上的逻辑。</p>
<h4 id="聚合层-Aggregation-Layer">聚合层(Aggregation Layer)</h4>
<p>在经过复杂的注意力计算后，模型需要把海量的数据“浓缩”成一个简洁的特征向量，供下游任务使用。</p>
<ul>
<li><strong>空间聚合：</strong> 利用聚合层 $\Phi_{Agg}$，将每一帧中散落在各个 Patch 里的信息，全部压缩到该帧的 <code>[cls]</code> 标记中（得到 $\hat{F}_i^{CLS}$）。</li>
<li><strong>最终表示 ($F_V$)：</strong> 将所有帧的 <code>[cls]</code> 标记拼接起来。
<ul>
<li><strong>结果：</strong> 得到的 $F_V$ 是一个矩阵，它每一行代表一帧的精华，整组矩阵代表了整个视频片段的精华。</li>
</ul>
</li>
</ul>
<h3 id="预训练层">预训练层</h3>
<h3 id="监督分类-Supervised-Classification-mathcal-L-sup"><strong>监督分类 (Supervised Classification, $\mathcal{L}_{sup}$)</strong></h3>
<ul>
<li><strong>做法：</strong> 将提取出的视频特征 $F_V$ 通过一个<strong>时间自注意力层</strong>，汇总到一个可学习的 <code>[cls]</code> 标记中。</li>
<li><strong>计算：</strong> 这个标记被输入线性分类器，使用<strong>交叉熵损失 (Cross-Entropy Loss)</strong> 进行训练。</li>
<li><strong>目的：</strong> 让模型学会“看图识事”，即直接根据画面判断这是进球还是犯规。</li>
</ul>
<h3 id="视频-语言对比学习-Video-Language-Contrastive-Learning-mathcal-L-contra"><strong>视频-语言对比学习 (Video-Language Contrastive Learning, $\mathcal{L}_{contra}$)</strong></h3>
<ul>
<li><strong>做法：</strong> 对视频特征进行平均池化得到 $F_V^{Avg}$，同时用文本编码器处理解说词 $C$。</li>
<li><strong>创新点：</strong> 借鉴了 <strong>SigLIP</strong> 的损失函数。</li>
<li><strong>正样本优化：</strong> 针对足球比赛中经常出现高度相似的解说（如“比赛开始”），模型将同一批次中相似度高的文本都视为正样本，增强了鲁棒性。</li>
<li><strong>目的：</strong> 建立视觉与文本的语义联系，为下游的解说生成任务打好基础。</li>
</ul>
<h3 id="疑问：">疑问：</h3>
<h4 id="为什么是监督分类？">为什么是监督分类？</h4>
<h3 id="1-足球语义的复杂性与明确性">1. 足球语义的复杂性与明确性</h3>
<ul>
<li><strong>语义明确：</strong> 足球比赛中的关键事件（如进球、黄牌、换人）都有非常明确的官方定义和边界。</li>
<li><strong>监督优势：</strong> 监督分类通过使用专家标注的 <strong>Event Labels</strong>，能直接“教会”模型识别这些高层语义特征。相比之下，无监督学习（如传统的聚类或掩码建模）可能只会让模型学会识别“草坪是绿色的”或“球员在跑动”，而难以自发理解“这是一个越位”这种复杂的逻辑关系。</li>
</ul>
<h3 id="2-预训练目标的互补性">2. 预训练目标的互补性</h3>
<p>根据文本，MatchVision 并不是只用监督学习，而是采用了<strong>混合策略</strong>：</p>
<ul>
<li><strong>监督分类 ($\mathcal{L}_{sup}$)</strong>：负责建立<strong>视觉特征与官方动作标签</strong>的强关联。</li>
<li><strong>对比学习 ($\mathcal{L}_{contra}$)</strong>：这其实具有一定的“弱监督”或“自监督”性质，它通过**视频与解说词（Textual Commentaries）**的匹配，让模型学习更丰富的语言描述能力。</li>
<li><strong>结合效果：</strong> 监督分类提供了“硬准则”（这是什么动作），而对比学习提供了“软语义”（这个动作怎么描述）。</li>
</ul>
<h3 id="3-提升特征的判别力-Discriminative-Power">3. 提升特征的判别力 (Discriminative Power)</h3>
<ul>
<li><strong>类内与类间差异：</strong> 足球视频中，很多动作看起来极其相似（例如，普通的传球和助攻传球在视觉上可能只有微小区别）。</li>
<li><strong>监督的作用：</strong> 使用交叉熵损失（Cross-Entropy Loss）的监督训练，会强制模型在特征空间中拉开不同事件类别的距离，从而在下游任务（如犯规识别）中表现得更精准。</li>
</ul>
<h3 id="4-行业数据集的现状">4. 行业数据集的现状</h3>
<ul>
<li><strong>标注资源：</strong> 足球领域拥有如 SoccerNet 这样大规模且高质量的标注数据集。</li>
<li><strong>效率考量：</strong> 既然已经有了现成的“正确答案（Labels）”，直接使用监督学习进行预训练，比让模型在海量无标注视频中漫无目的地探索（无监督）要高效得多。</li>
</ul>
<h4 id="什么是cls和cmt">什么是cls和cmt</h4>
<h2 id="1-CLS-Event-Classification-事件分类">1. CLS (Event Classification - 事件分类)</h2>
<p><strong>CLS</strong> 是 <strong>Classification</strong> 的缩写，主要负责“看图识事”，即识别视频中发生了什么特定事件。</p>
<ul>
<li><strong>核心功能：</strong> 将输入的足球视频片段归类为预定义的事件标签，例如“进球”、“角球”、“黄牌”或“换人”。</li>
<li><strong>实现机制：</strong> * 模型会引入一个可学习的 <strong>[cls] token</strong>，通过<strong>时间自注意力机制</strong>（Temporal Self-attention）汇总整段视频的时空特征。
<ul>
<li>该特征随后被送入一个<strong>线性分类器</strong>（Linear Classifier）进行处理。</li>
</ul>
</li>
<li><strong>输出结果：</strong> 给出各个事件类别的概率分布，通常选取概率最高的一个作为最终判定结果（如：Type: “Yellow card”）。</li>
</ul>
<hr>
<h2 id="2-CMT-Commentary-Generation-解说生成">2. CMT (Commentary Generation - 解说生成)</h2>
<p><strong>CMT</strong> 是 <strong>Commentary</strong> 的缩写，主要负责“见图说话”，即生成像专业解说员一样的自然语言描述。</p>
<ul>
<li><strong>核心功能：</strong> 自动为视频片段编写一段符合赛况的叙述性文字。</li>
<li><strong>实现机制：</strong>
<ul>
<li>使用 <strong>Perceiver 聚合器</strong> 将复杂的视觉特征浓缩，并通过 <strong>MLP</strong> 映射为<strong>前缀嵌入</strong>（Prefix Embeddings）。</li>
<li>这些视觉嵌入被输入到**大语言模型（LLM）**中，引导 LLM 根据画面内容生成文本。</li>
</ul>
</li>
<li><strong>输出结果：</strong> 一段完整的句子，例如：“[REFEREE] shows a yellow card to [PLAYER]…”。</li>
</ul>
<h3 id="下游任务层">下游任务层</h3>
<p>预训练完成后，视觉编码器被“冻结”或作为骨干网络，通过不同的<strong>预测头 ($\Psi$)</strong> 来适配具体任务：</p>
<h3 id="事件分类-Psi-cls"><strong>事件分类 ($\Psi_{cls}$)</strong></h3>
<ul>
<li><strong>机制：</strong> 结构与预训练的监督学习类似，使用时间自注意力聚合特征。</li>
<li><strong>训练逻辑：</strong> 在<strong>冻结视觉编码器</strong>的情况下，仅训练线性分类器。</li>
<li><strong>输出：</strong> 给出视频属于哪种事件（如：角球、黄牌）的概率分布。</li>
</ul>
<h3 id="解说生成-Psi-Cmt"><strong>解说生成 ($\Psi_{Cmt}$)</strong></h3>
<ul>
<li>
<p><strong>核心组件：</strong> <strong>Perceiver 聚合器</strong> + <strong>MLP</strong> + <strong>LLM（大语言模型）</strong>。</p>
</li>
<li>
<p>流程： 1. Perceiver 将海量的视觉特征压缩。</p>
<p>\2. MLP 将其映射为 LLM 能听懂的“前缀嵌入（Prefix Embeddings）”。</p>
<p>\3. LLM 根据这些“视觉前缀”像写作文一样生成解说词。</p>
</li>
<li>
<p><strong>损失函数：</strong> 使用负对数似然损失（Next-Token Prediction）。</p>
</li>
</ul>
<h3 id="犯规识别-Psi-Foul"><strong>犯规识别 ($\Psi_{Foul}$)</strong></h3>
<ul>
<li><strong>输入：</strong> 足球比赛中常见的**多视角（Multi-view）**视频。</li>
<li><strong>处理：</strong> 使用池化技术（Max/Avg Pooling）将多视角特征整合为一个向量。</li>
<li><strong>双任务输出：</strong> 使用一个共享的 MLP 接两个分类器，同时预测：
<ol>
<li><strong>犯规类型</strong>（如：铲球犯规、手球等，共 8 种）。</li>
<li><strong>严重程度</strong>（如：口头警告、黄牌、红牌等，共 4 级）。</li>
</ol>
</li>
</ul>
<p><strong>为什么要使用MLP</strong></p>
<p>实现跨模态的特征对齐，不需要更强大的模型，简单的MLP足够胜任模态对齐工作</p>
<h3 id="实验部分">实验部分</h3>
<p>基于他上面自己的soccer Replay 1988数据集进行实验</p>
<p>MatchVision在分类这个任务是达到了**82.5%**的准确率</p>
<p>证明对比学习比监督学习的效果更好</p>
<p>并且MatchVision在foul recongition方面，即使冻结了视觉编码器，也和顶尖模型不相上下</p>
<h3 id="最后部分">最后部分</h3>
<p>使用了LoRA技术调教LLM</p>
<p>这篇论文有三个比较大的贡献</p>
<p><strong>新资源</strong>：造出了迄今为止最大的足球数据集 <strong>SoccerReplay-1988</strong>。</p>
<p><strong>新模型</strong>：开发了专门针对足球时空特征的编码器 <strong>MatchVision</strong>。</p>
<p><strong>新标杆</strong>：在分类、解说、犯规识别等多个任务上都达到了<strong>世界领先水平 (SOTA)</strong>。</p>
<h2 id="Multi-Agent-System-for-Comprehensive-Soccer-Understanding">Multi-Agent System for Comprehensive Soccer Understanding</h2>
<h3 id="引言">引言</h3>
<p>论文在引言部分介绍了现在的研究在足球理解研究的一些挑战</p>
<p>在推理任务上比较的局限（局限于视觉分析而缺少了推理）</p>
<p>以及模型过于的碎片化和专家化</p>
<p><strong>这篇文章主要有四个贡献</strong></p>
<p><strong>构建了 SoccerWiki 知识库</strong>：这是第一个大规模的多模态足球知识库，集成了关于球员、球队、裁判和场地的丰富领域知识，旨在支持知识驱动的推理任务 。该库包含 9,471 名球员、266 支球队、202 名裁判和 235 个场地的详细信息 。</p>
<p><strong>建立了 SoccerBench 基准测试集</strong>：这是目前最大且最全面的足球领域专项基准 。它通过自动化的数据策划和人工验证构建，包含约 1 万个多模态（文本、图像、视频）选择题对，涵盖了背景知识、比赛局势识别、犯规识别等 13 项不同的足球分析任务 。</p>
<p><strong>开发了 SoccerAgent 多智能体系统</strong>：这是一种新型的多智能体协作系统，通过将复杂的足球问题分解为多个可执行的子任务来解决问题 。它利用了 SoccerWiki 的领域专家知识，并能够调用 18 个专项工具进行协作推理 。</p>
<p><strong>进行了广泛的评估与对比</strong>：作者在 SoccerBench 上将 SoccerAgent 与 11 种代表性的多模态大语言模型（MLLMs，如 GPT-4o、Claude 3.7、Gemini 2.0 等）进行了深入对比 。评估结果突显了该智能体系统在处理复杂、知识密集型足球任务中的优越性 。101</p>
<h3 id="介绍soccerBench">介绍soccerBench</h3>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>包含任务 (Index)</strong></th>
<th><strong>考查重点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>纯文本推理</strong> (TextQA)</td>
<td><strong>Q1</strong> 背景知识, <strong>Q2</strong> 比赛局势</td>
<td>考查模型是否掌握了球员历史、转会、比赛战术等“足球常识”。</td>
</tr>
<tr>
<td><strong>图像视觉感知</strong> (ImageQA)</td>
<td><strong>Q3</strong> 相机状态分类, <strong>Q4</strong> 图片背景知识, <strong>Q5</strong> 球衣号码识别, <strong>Q6</strong> 比分与时间识别</td>
<td>考查模型对单张转播截图的解析力，例如识别“这是哪场比赛”、“这是几号球员”。</td>
</tr>
<tr>
<td><strong>视频动态分析</strong> (VideoQA)</td>
<td><strong>Q7</strong> 相机切换, <strong>Q8</strong> 回放定位, <strong>Q9</strong> 动作分类, <strong>Q10/Q11</strong> 评论生成与理解, <strong>Q12</strong> 球衣颜色识别, <strong>Q13</strong> 多视角犯规识别</td>
<td><strong>最难的部分</strong>。考查模型能否理解动作的连贯性，并根据规则做出裁判级别的判断（如 Q13 判定是否犯规）。</td>
</tr>
</tbody>
</table>
<h3 id="研究动机">研究动机</h3>
<p>作者认为目前足球AI时效性不足，评价碎片化</p>
<p>作者构建了SoccerWIKI，并且在此基础上构建了SoccerBench</p>
<h4 id="Data-Curation">Data Curation</h4>
<p>团队采用不同的策略生成原始问答对（模版生成，大模型生成）</p>
<p>并且转化成四选一的选择题</p>
<p>最后通过自动化合成再经过人工筛选，组成了<strong>SoccerBench</strong></p>
<h3 id="SoccerAgent">SoccerAgent</h3>
<p><img src="/2025/12/23/%E5%88%AB%E5%86%8D%E8%AF%B4-AI-%E4%B8%8D%E6%87%82%E7%90%83%EF%BC%9AMatchTime-%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%B7%B1%E5%BA%A6%E7%AC%94%E8%AE%B0/C:%5CUsers%5Cyunan%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20251225220605344.png" alt="image-20251225220605344"></p>
<p>论文的核心部分</p>
<h4 id="基于DeepSeek-V3的主模块协同工作">基于DeepSeek-V3的主模块协同工作</h4>
<p><strong>规划者 ($\mathcal{A}_{plan}$)</strong>：负责“思考”。它接收问题后，并不直接回答，而是分析需要哪些步骤，从工具包里挑选出最合适的<strong>工具链</strong>。</p>
<p><strong>执行者 ($\mathcal{A}_{exec}$)</strong>：负责“动手”。它按照规划好的顺序，一个接一个地运行工具。每一步都会参考之前的执行历史（$\mathcal{H}_{i}$），从而实现上下文感知的自适应调整。</p>
<h4 id="ToolBox">ToolBox</h4>
<p><strong>12 个足球专项工具</strong>：</p>
<p><strong>基础分析</strong>：动作分类器、评论生成 。</p>
<p><strong>检索专家</strong>：比赛搜索、比赛历史/信息检索、<strong>人脸识别</strong>（从 SoccerWiki 匹配球员） 。</p>
<p><strong>感知专家</strong>：相机状态检测、<strong>球衣号码/颜色识别</strong>、比分和时间识别 。</p>
<p><strong>高级裁判</strong>：<strong>犯规识别</strong>（通过多视角投票机制模拟 VAR）和回放定位 。</p>
<p><strong>6 个通用解析工具</strong>：</p>
<p>包括<strong>帧选择</strong>（将视频转为关键帧）、语义分割（定位特定物体）、实体搜索和文本检索等 。</p>
<h3 id="实验部分-2">实验部分</h3>
<p>比较重点的：我认为是容错能力</p>
<p><strong>自主调整：</strong> 执行者 ($\mathcal{A}_{exec}$) 在发现第一步失败后，并没有卡死，而是根据历史上下文自主调整策略，改用“比赛搜索”工具成功找回了所需信息 。</p>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 Izumi Sagiri
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Izumi Sagiri
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
